In this section, we show that the general framework presented above can be used to a specific class of portfolio selection problem, namely the Risk Parity portfolio selection. We use volatility as risk measure of the fully invested portfolio, i.e.
\begin{equation}
\mathcal{R}(x) = \sigma(x) = \sqrt{x^T Q x}
\end{equation}
where $Q$ is the covariance matrix. Using the Euler decomposition, we can express the total risk as the sum of contributions from each asset in the portfolio:
\begin{equation}
\mathcal{R}(x) = \sum_{i=1}^n RC_i 
\end{equation}
where $RC_i$ is the risk contribution of the $i$-th asset, that has the form
\begin{equation}
RC_i = x_i \frac{\partial \mathcal{R}(x)}{\partial x_i}
\end{equation}
In the Risk Parity formulation, our aim is to satisfy the following set of constraints:
\begin{equation}\label{eq:rpconst}
x_i \frac{\partial \mathcal{R}(x)}{\partial x_i}= x_j \frac{\partial \mathcal{R}(x)}{\partial x_j} \quad \forall i,j
\end{equation}
We can also express (\ref{eq:rpconst}) in the following equivalent way:
\begin{equation}
x_i (Q x)_i = x_j (Q x)_j \quad \forall i,j
\end{equation}
In \cite{maillard} is proposed a least-square approach for solving the Risk Parity problem:
\begin{subequations}
\begin{align}
\min_x & \quad \sum_{i=i}^n \sum_{j=1}^{n}\left(x_i(Q x)_i - x_j(Q x)_j\right)^2\\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathds{1}^T x = 1 
\end{align}
\end{subequations}
The formulation proposed in \cite{tutuncu} introduces a free variable $\theta$ that is also optimized:
\begin{subequations}\label{eq:problemRP} 
\begin{align}
\min_{x,\theta} & \quad f(x,\theta) =  \sum_{i=i}^n \left(x_i(Q x)_i - \theta\right)^2 \\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathds{1}^T x = 1 
\end{align}
\end{subequations}
where $x \in \R^n$, $\theta \in \R$, $f$ continuously differentiable, $l, u \in \R^n$ with $l < u$ and $\mathds{1} \in \R^n$ is all composed by ones. \\
It is easy to see that Problem (\ref{eq:problemRP}) is a specific case of Problem (\ref{eq:problem}). Note that the formulation (\ref{eq:problemRP}) is non-convex, but $f(x,\theta)$ is quadratic and strictly convex with respect to $\theta$.  If the optimization problem above has an optimal value of zero, then the RP portfolio is achieved. Otherwise, the value of $\sum_{i=i}^n \left( x_i(Q x)_i - \theta\right)^2$  can be regarded as a minimum variance measure towards our goal. Note that, since (\ref{eq:problemRP}) is a non-convex problem, in theory it is hard to solve and may produce local solutions. Anyway, we have the following lemma (for the proof see \cite{tutuncu}):
\begin{lemma}\label{rplemma}
Let $f(x,\theta) = \sum_{i=i}^n \left( x_i(Q x)_i - \theta\right)^2$. A solution pair $\{x,\theta\}$ is a global optimum with $f(x,\theta)=0$ if and only if $\nabla_xf(x,\theta) = 0$ and $\frac{\partial f(x,\theta)}{\partial \theta} = 0$.
\end{lemma}
Lemma (\ref{rplemma}) implies that if constraints of (\ref{eq:problemRP}) are not considered, then the first order optimality conditions determine the global optimal solution. On the other hand, when constraints are imposed, local optima and local stationary points can occur.
\subsection{Proximal Point modification}
In this subsection we propose a \emph{Risk-Parity} version of general decomposition algorithm described above, using proximal point approximation.

Let us redefine selected variable $x_{i(k)},x_{j(k)}$ as $x_{i},x_{j}$ for ease of notation, and let us consider the step when $\theta$ is fixed (i.e $f(x,\theta)=f(x)$).

The idea behind proximal point modification is that at every iteration $k$, fixed other variable $\theta$, it's easy to solve the subproblem:
\begin{align}
 &\min_{x_i,x_j}h(x_i,x_j)= f(x_i,x_j)+ \frac{1}{2} \tau||(x_i,x_j)-(x_i^k,x_j^k)||^2\\
 &x_i+x_j = \underbrace{1-\sum_{h \ne i,j} x^k_h}_{c^k}\\
 &l \le x_i,x_j\le u
 \end{align}
where $f$ is defined in \ref{eq:problemRP} and $\tau>0$.

In fact, due to simplex constraint, the objective function depends only on one of the selected variables (e.g. $x_j$) and $f$ become 4-degree polynomial in $x_j$.

Because of $h$ is continuously differentiable, it admits minimum in a compact set and we have to search it between zeros of $h'(\xi)$ and the limit points of the feasible set.

Hence, at every iteration $k$, we can define the set of possible global minima as:
\begin{equation}
 O_k = \{ \xi < x_j^k: h'(\xi)=0\} \cup \{\min\{l_j,x_i^k\} \}
\end{equation}

then we set:
\begin{equation}
x_j^{k+1}= \arg \min_{\xi \in O_k} \{h(\xi)\}
\end{equation}
and:
\begin{equation}
x_i^{k+1}= c^{k}-x^{k+1}_j
\end{equation}



\begin{algorithm}[ht]
 \KwData{Given the initial feasible point $(x^{0}, \theta^{0})$}
 Set $k = 0$\\
 \While{(not convergence)}{
   Compute $\theta^{k+1}$ such that $f(x^{k},\theta^{k+1})\le f(x^{k},\theta^k)$ and $\nabla_\theta f(x^{k},\theta^{k+1})=0$\\
 \eIf{$k \enskip \text{mod} \enskip M = 0$}
  {
  Let $(i(k), j(k))$ be a Most Violating Pair\\ 
  }
  {
  Let $(i(k), j(k))$ be a Violating Pair\\ 
  }
  Compute $\displaystyle x_{i(k)},x_{j(k)}\in \arg \min_{\xi,\zeta} f(\xi,\zeta)+\frac{1}{2}\tau ||(\xi,\zeta)-(x_{i(k)}^k,x_{j(k)}^k)||^2$\\
  Set $k = k + 1$
 }
 \caption{Decomposition Algorithm with proximal point}
\end{algorithm}
We can prove the follwing global convergence results.
\begin{proposition}
Suppose that the level set $\mathcal{L}_0$ is a compact set. Let $\{(x^k, \theta^k)\}$ be the sequence of points generated by the decomposition algorithm with proximal point modification. Then
\begin{itemize}
\item $(x^k, \theta^k) \in \mathcal{L}_0, \enskip \forall k$ 
\item  $\{(x^k, \theta^k)\}$ admits limit points and each limit point is critical for Problem (\ref{eq:problemRP})
\end{itemize}
\end{proposition}
\begin{proof}
The firts assertion is given by algorithm statement which say:
\begin{equation}
f(x^{k+1},\theta^{k+1})\le f(x^{k+1},y^k)\le f(x^k,\theta^k)- \underbrace{\frac{1}{2}\tau ||x^{k+1}-x^{k}||^2}_{>0}
\end{equation}
then the sequence $\{(x^k,\theta^k)\} \in \mathcal{L}_0$.

To prove the second assertion it is necessary to show that
\begin{equation}
\lim_{k \rightarrow \infty} ||(x^{k+1},\theta^{k+1})-(x^{k},y^{k})||=0
\end{equation}
then the convergence can be completed as in previous proposition.

Let us start to prove that $||x^{k+1}-x^{k}||\to0$.

From the $\mathcal{L}_0$ compactness we can say that the sequence $\{(x^k,\theta^k)\}$ has a subsequence $K \subset \{0,1,\ldots\}$ convergent to $(\overline{x},\overline{y}) \in \mathcal{L}_0$. Then for the continuity of $f$ we have:
\begin{equation}
\lim_{k \in K, k \rightarrow \infty}f(x^k,\theta^k)=f(\overline{x},\overline{\theta})= \overline{f}
\end{equation}

The instruction of algorithm imply that:
\begin{equation}
f(x^{k+1},\theta^{k+1})\le f(x^{k+1},\theta^{k})\le f(x^k,\theta^k)
\end{equation}
then the sequence $f(x^k,\theta^k)$ is decreasing and lower-bounded by $\overline{f}$ then we have:
\begin{equation}
\lim_{k \in K, k \rightarrow \infty} f(x^{k+1},\theta^{k})-f(x^{k},\theta^{k})=0
\end{equation}

Proximal point step imply that:
\begin{equation}
f(x^{k+1},y^{k})+\frac{1}{2}\tau||x^{k+1}-x^{k}||^2 \le f(x^{k},y^{k})
\end{equation}

Taking the limit for $k \in K, k \to \infty$ we obtain:
\begin{equation}
\lim_{k \in K,k \to \infty} ||x^{k+1}-x^{k}||=0
\end{equation}

Now we have to prove that $|\theta^{k+1}-\theta^{k}|\to 0$, using the property that $f$ is quadratic and strictly convex with respect to $\theta$. 
We have to prove the following lemma:
\begin{lemma}
Let $f(x, \theta)$ defined in (\ref{eq:problemRP}). Then, if $\enskip\lim_{k \to \infty} f(x^{k},\theta^k)-f(x^{k},\theta^{k+1})=0$ then:
\begin{equation}
 \lim_{k\to \infty} |\theta^{k+1} -\theta^k| = 0 
\end{equation}
\end{lemma}

\begin{proof}
The instructions of the algorithm imply that at every iteration $k$, we have to select the optimal step $\alpha_k^*$ along $-\nabla_{\theta}f(x^{k},\theta^{k})$.

Because of $f$ is strictly convex quadratic function with respect to $\theta$ we have:
\begin{equation}\label{eq:quadratic}
f(x^{k},\theta^k-\alpha^*_k\nabla_{\theta}f(x^{k},\theta^{k})) = f(x^{k},\theta^{k})-\alpha^*_k |\nabla_{\theta}f(x^{k},\theta^{k})|^2
\end{equation}
with:
\begin{equation}\label{eq:upTheta}
 \theta^{k+1}= \theta^{k}-\alpha^*_k\nabla_{\theta}f(x^{k},\theta^{k})
\end{equation}

From hypothesis we have that:
\begin{equation}
 \lim_{k \to \infty} f(x^{k+1},\theta^k)-f(x^{k+1},\theta^{k+1})=0
 \end{equation}

and from \ref{eq:upTheta} and \ref{eq:quadratic} :
\begin{equation}
  \lim_{k\to \infty} |\theta^{k+1} -\theta^k| = 0 
\end{equation}
\end{proof}
 
Next convergence steps are the same used in the proof of Proposition \ref{prop:conv1}.
\end{proof}

\begin{oss}
In computational experiments we use $\tau = 0$ and compute an exact line search along direction $d^{i(k),j(k)}$. In next section we will show that in practical cases exact line search performs very well especially increasing number of variables.
\end{oss}


