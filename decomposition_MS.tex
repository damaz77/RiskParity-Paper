As already discussed, we partition the vector of variables into two blocks in order to take into account
the structure of the feasible set and possibly the form of the objective function (see, for instance,
the formulation of the risk parity problem, where the objective function is convex w.r.t. the scalar variable $\theta$).
The first block contains the constrained variables $x$, the second block contains
the unconstrained variables $y$. 

A first possibility can be that of defining a two-blocks Gauss-Seidel algorithm.
According to this scheme, at each iteration, the two component vectors $x$ and $y$ are
sequentially updated by performing  minimization steps (either exact or inexact) by  suitable descent techniques.
Globally convergent results of Gauss-Seidel algorithms (both exact and inexact) have been established in \cite{}, \cite{}, \cite{}.

We present here a block descent algorithm where a further level of decomposition is
introduced with respect to the block component $x$. 
More specifically, at each iteration, only two variables are updated, those corresponding
to a {\it Violating Pair}, by performing an inexact line search along a feasible and descent direction.
In order to guarantee convergence, the {\it Most Violating Pair} must be selected at least periodically, say every $M$ iterations.
We will discuss in the section of the computational experiments the role and the influence of the parameter $M$.

The adoption of a decomposition strategy with respect to the subvector $x$ is suitable
whenever the number $n$ of variables is large.

Note that the properties of the standard Armijo-type line search do not guarantee, without further assumptions
on the descent search direction $d^k$, that the distance between successive points tends to zero, which is a usual requirement of decomposition methods. 
This motivates the employment of the Quadratic Line Searck (QLS) defined in the appendix and based on the acceptance condition
$$
f((x^k,y^k)+\alpha^kd^k)\le f((x^k),y^k))-\gamma (\alpha^k)^2\|d^k\|^2.
$$
Concerning the unconstrained block component $y$, we do not specify the updating rule, but we state the following assumption that
could be satisfied, in practice, by different techniques depending on the hypothesis on $f$.
\par\medskip\noindent
{\bf Assumption on the updating rule of} $y$.
\par\medskip\noindent
\begin{itemize}
\item[(i)] For each $k$ we have $f(x^k,y^{k+1})\le f(x^k,y^k)$
\item [(ii)] If
$$
\lim_{k\to\infty} \left(f(x^k,y^{k+1})- f(x^k,y^k)\right)=0
$$
then
$$
\lim_{k\to\infty}\|y^{k+1}-y^k\|=0
$$
and
$$
\lim_{k\to\infty}\nabla_y f(x^k,y^k)=0.
$$
\end{itemize}


%At every step $k$ we choose a random subset $W^k\subset \{1,..,n\}$ such as
%\begin{equation}\label{eq:lambda}
%\frac{|W^k|}{n} \times 100 = \lambda
%\end{equation}
%For each $w \in W^k$, we compute the partial derivative $\frac{\partial f(x,\theta)}{\partial x_w}$ and 
%we select the MVP among the indexes in $W^k$. If we don't find a violating pair in $W^k$, 
%we randomly add indexes until we find one, and we use this violating pair to build the descent direction. To assure the global convergence properties, we evaluate the MVP among the full gradient $\nabla_x f(x,\theta)$ every $M$ iterations. \\
The algorithm is formally described below.

\begin{algorithm}[ht]
 \KwData{Given the initial feasible point $(x^{0}, y^{0})$}
 Set $k = 0$\\
 \While{(not convergence)}{
  Compute $y^{k+1}$ such that Assumptions (i) and (ii) hold\\
 \eIf{$k \enskip \text{mod} \enskip M = 0$}
  {
  Let $(i(k), j(k))$ be a Most Violating Pair\\ 
  }
  {
  Let $(i(k), j(k))$ be a Violating Pair\ 
  }
  Compute a step $\alpha^{k}$  along the direction $d^k=d^{i(k),j(k)}$ by QLS\\
  Set $x_{i(k)}^{k+1} = x_{i(k)}^{k} + \alpha^{k}$, $x_{j(k)}^{k+1} = x_{j(k)}^{k} - \alpha^{k}$  \\

  Set $k = k + 1$
 }
 \caption{Decomposition Algorithm}
\end{algorithm}
\par\bigskip\noindent
We can prove the following global convergence result.
\begin{proposition}
Suppose that the level set $\mathcal{L}_0$ is a compact set. Let $\{(x^k, y^k)\}$ be the sequence of points generated by the decomposition algorithm. Then
$\{(x^k, y^k)\}$ admits limit points and each limit point is critical for Problem (\ref{eq:problem}).
%\end{itemize}
\end{proposition}

\begin{proof}
The instructions of the algorithm imply
$$
f(x^{k+1}, y^{k+1})\le f(x^{k}, y^{k+1}) \leq f(x^{k}, y^{k}),
$$
so that, the points of the sequence $\{(x^{k}, y^{k})\}$ belongs to the compact set $\mathcal{L}_0$.
We also have
\begin{equation}\label{on_funct}
 \lim_{k\to\infty} f(x^k,y^{k+1})=\lim_{k\to\infty} f(x^{k},y^{k})=\bar f>-\infty
\end{equation}
Let $(\overline{x},\overline{y})$ be a limit point of $\{(x^k, y^k)\}$, i.e. there exists an infinite subset $K \subseteq N$ such that
\begin{equation}\label{eq:asim}
\lim_{k \in K, k \rightarrow \infty} (x^k, y^k) = (\overline{x},\overline{y})
\end{equation}
By contradiction, let us assume that $(\overline{x},\overline{y})$ is not a critical point. 
In this case, at least one of the following conditions holds:
\begin{subequations}
\begin{align}
&\nabla_y f(\overline{x},\overline{y}) \neq 0  \label{eq:a}\\
&\exists \enskip i, j \enskip  \text{s.t.} \enskip d^{i,j}\in D(\bar x) \enskip  \text{and} \enskip  \nabla_x f(\overline{x},\overline{y})^T d^{i,j} = -\eta < 0 \label{eq:b}
\end{align}
\end{subequations}
Suppose that (\ref{eq:a}) holds.
From (\ref{on_funct}), Assumption (ii) on the updating rule of $y^k$, and the continuity of the gradient we get
$$
\lim_{k\to\infty}\nabla_y f(x^k,y^k)=\nabla_y f(\overline{x},\overline{y})=0,
$$
and this contradicts (\ref{eq:a}).

Now assume that (\ref{eq:b}) holds.
For each $k$, a stepsize $\alpha^k>0$ is computed by QLS along the descent direction $d^{i(k),j(k)}$.
Then we can write
\begin{equation}\label{red_funct}
 f(x^{k+1},y^{k+1})\le f(x^k,y^{k+1})-\gamma (\alpha^k)^2\|d^{i(k),j(k)}\|^2=f(x^k,y^{k+1})-\gamma\|x^{k+1}-x^k\|^2,
\end{equation}
from which, recalling (\ref{on_funct}) and that $\|d^{i(k),j(k)}\|^2=2$, we obtain
\begin{equation}\label{dst_x}
 \lim_{k\to\infty}\|x^{k+1}-x^k\|=\lim_{k\to\infty}\alpha^k=0.
\end{equation} 
From (\ref{on_funct}) and Assumption (ii) on the updating rule of $y^k$ we also have
\begin{equation}\label{dst_y}
 \lim_{k\to\infty}\|y^{k+1}-y^k\|=0.
\end{equation}
For each $k\in K$, let $v(k)$ be the integer such that $k+v(k)$ is an iteration
where the Most Violating Pair is selected. Note that we have
$$
0\le v(k) \le M.
$$
From (\ref{dst_x}) and (\ref{dst_y}) we obtain
\begin{equation}\label{cnv_x}
 \lim_{k\in K,k\to\infty}x^{k+v(k)}=\bar x
\end{equation}
\begin{equation}\label{cnv_y}
 \lim_{k\in K,k\to\infty}y^{k+v(k)+1}=\bar y.
\end{equation}
We introduce the index set 
$$
K_1=\{h: \ h=k+v(k), \ k\in K\}.
$$
By definition, for all $k\in K_1$ the Most Violating Pair $(i(k),j(k))$ is selected. Furthermore, we have
\begin{equation}\label{cnv2_x}
 \lim_{k\in K_1,k\to\infty}x^{k}=\bar x
\end{equation}
\begin{equation}\label{cnv2_y}
 \lim_{k\in K_1,k\to\infty}y^{k+1}=\bar y.
\end{equation}
Since $i(k)$ and $j(k)$ belong to the finite set $\{1,\ldots ,n\}$, we can extract a further subset (that we relabel by $K_1$) such that
$$
i(k)=i^\star \quad\quad j(k)=j^\star\quad\quad \forall k\in K_1.
$$
Note that $d^{i,j}\in D(\bar x,\bar y)$, so that, recalling Proposition \ref{2.3}, we obtain for
$k\in K_1$ and $k$ sufficiently large
\begin{equation}\label{dij_dk}
 d^{i,j}\in D(x^k,y^k).
\end{equation}
For all $k\in K_1$, as $(i^\star,j^\star)$ is the Most Violating Pair, using (\ref{dij_dk}) we can write
\begin{equation}\label{mvp_ij}
 \nabla_xf(x^k,y^{k+1})^Td^{i^\star,j^\star}\le \nabla_xf(x^k,y^{k+1})^Td^{i,j}<0.
\end{equation}
Then, $d^{i^\star,j^\star}$ is a feasible and descent direction, and hence a stepsize $\alpha^k$ is computed along it by QLS.
Further, from (\ref{dst_y}) and (\ref{eq:b}) it follows
\begin{equation}\label{desc_mvp}
 \nabla_xf(\bar x,\bar y)^Td^{i^\star,j^\star}<0.
\end{equation}
Now let us distinguish two cases:
\par\medskip\noindent
Case (I). There exists an integer $\ell$  such that
\begin{equation}\label{caseI}
 \alpha^{k+\ell(k)}<\Delta^{k+\ell(k)}
\end{equation}
for all $k\in K_1$ and for some $\ell(k)\le \ell$.
\par\medskip\noindent 
Case (II). For all $k\in K_1$ and $m=0,\ldots ,2n$ we have
\begin{equation}\label{caseII}
 \alpha^{k+m}=\Delta^{k+m}.
\end{equation}
{\it Case} (I)
\par\medskip\noindent
Without loss of generality assume $\ell (k)=0$.
Otherwise we can reason on the subsequence $\{x^k\}_{k+l(k)}$ that, by definition,
is a subsequence where the  MVP is selected again (see the new instruction) since $\alpha^{k+l(k)-s}=\Delta^{k+l(k)-s}$,
for $s=0,1,\ldots ,l(k)$,
and $\alpha^{k+l(k)}<\Delta^{k+l(k)})$.

The instructions of QLS imply for all $k\in K_1$
\begin{equation}\label{eq:arm1}
f(x^k + \frac{\alpha^k}{\delta} d^{i^\star,j^\star}, y^{k+1}) - f(x^k,y^{k+1}) > -2\gamma \left(\frac{\alpha^k}{\delta}\right)^2 
\end{equation}
Using the Mean Value Theorem, we can write
\begin{equation}\label{eq:arm2}
f(x^k + \frac{\alpha^k}{\delta} d^{i^\star,j^\star}, y^{k+1}) = f(x^k, y^{k+1}) + \frac{\alpha^k}{\delta} \nabla_x f(z^k, y^{k+1})^T d^{i^\star,j^\star}
\end{equation}
where $z^k = x^k + \vartheta_k \frac{\alpha^k}{\delta} d^{i^\star,j^\star}$ and $\vartheta_k \in (0,1)$. 
From (\ref{eq:arm2}) and (\ref{eq:arm1}), for $k\in K_2$ and $k$ sufficiently large we have
\begin{equation}\label{eq:nabla}
\nabla_x f(z^k, y^{k+1})^T d^{i^\star,j^\star}  >- 2\gamma \frac{\alpha^k}{\delta} 
\end{equation}
Using (\ref{dst_x}) and (\ref{dst_y}) and we obtain
$$
\lim_{k \in K_1, k \rightarrow \infty} z^k = \lim_{k \in K_1, k \rightarrow \infty} x^k + \vartheta_k \frac{\alpha^k}{\delta} d^{i^\star,j^\star} = \overline{x}
$$
$$
\lim_{k\in K_1,k\to\infty}y^{k+1}=\bar y.
$$
Taking the limits in (\ref{eq:nabla}) for $k\in K_1$ and $k\to\infty$ we have
\begin{equation}\label{ddirmg0}
\nabla_x f(\overline{x},\overline{y})^T d^{i^\star,j^\star} \geq 0,
\end{equation}
which contradicts (\ref{desc_mvp}).
\par\bigskip\noindent
{\it Case} II.
\par\medskip\noindent
For all $m=0,\ldots ,2n$ we have that at least one of these two possible cases hold
 \begin{subequations}
\begin{align}
 i(k+m)&\in R(x(k+m))\quad \quad i(k+m)\notin R(x(k+m+1))\label{eq:SetCasesA}\\
 j(k+m)&\in S(x(k+m))\quad \quad j(k+m)\notin S(x(k+m+1))\label{eq:SetCasesB}
\end{align}
\end{subequations}
Now we define two sets $\Gamma_1,\Gamma_2$ in $\{x^{k}\}_{k \in K}$ such that the first one contains all indexes $m \in\{0,\ldots,2n\}$ such that 
(\ref{eq:SetCasesA}) holds.
The second one contains all indexes $m \in\{0,\ldots,2n\}$ such that (\ref{eq:SetCasesB}) holds.

Since $|\Gamma_1|+|\Gamma_2|\ge 2n+1$, one of these sets contains a number of elements greater than $n$. 
Without loss of generality assume $|\Gamma_1|> n$.
We can say that there exist $ \hat i \in \{1,\ldots,n\}$ and $l(k),m(k)$ such that:
 \begin{equation}
  k\le l(k) <m(k)\le 2 n
 \end{equation}
and
\begin{equation}
 i(k) = i(l(k))=i(m(k))=\hat i.
\end{equation}
We can define a subset $K_1 \subseteq K$ such that $\forall k_i \in K_1$ we have
\begin{equation}
 i(k_i)=\hat i
\end{equation}
and
\begin{equation}
 k_i <k_{i+1} \le k_i+2n
\end{equation}
From the MVP rule it follows
\begin{equation}
 \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{\hat i}} \le \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{h}}, \ \forall h \in R(x^{k_i})
\end{equation}
For all $k_i \in K_1$, there exists $p(k_i)$, with  $k_i <p(k_i)<k_{i+1}$, such that
\begin{equation}
 \hat i \not \in R(x^{p(k_i)})\quad\quad \hat i \in \in R(x^{p(k_i)+1}).
\end{equation}
Then, again from the MVP rule, we must have
\begin{equation}
 \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{\hat i}} \ge \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{h}}, \ \forall h \in S(x^{p(k_i)})
\end{equation}
From (\ref{dst_x}) and (\ref{dst_y}), recalling that  $p(k_i)-k_i \le 2n$, we have
$$
 \lim_{k_i\rightarrow \infty} x^{p(k_i)}=\overline{x}
$$
$$
 \lim_{k_i\rightarrow \infty} y^{p(k_i)+1}=\overline{y}
$$
Note that $d^{i,j}\in D(\bar x,\bar y)$, so that, recalling Proposition \ref{2.3}, we obtain for
$k\in K_1$ and $k$ sufficiently large
\begin{equation}\label{dij_dka}
 d^{i,j}\in D(x^{k_i},y^{k_i+1}),
\end{equation}
\begin{equation}\label{dij_dkbis}
 d^{i,j}\in D(x^{p(k_i)},y^{p(k_i)+1}),
\end{equation}
i.e.,
$$
i\in R(x^{k_i})\quad\quad j\in S(x^{k_i})
$$ 
$$
i\in R(x^{p(k_i)})\quad\quad j\in S(x^{p(k_i)}).
$$
Then  we can write
\begin{equation}\label{eq:direction1}
\begin{aligned}
 \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{\hat i}} &\le \frac{\partial f(x^{k_i},y^{k_i+1})}{\partial x_{i}}\\
 \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{\hat i}} &\ge \frac{\partial f(x^{p(k_i)},y^{p(k_i)+1})}{\partial x_{j}}
 \end{aligned}
\end{equation}
Taking the limits for $k_i\in K_1$ and $k_i\to\infty$ and recalling the continuity of the gradient we obtain
$$
 \frac{\partial f(\overline{x},\overline{y})}{\partial x_i} - \frac{\partial f(\overline{x},\overline{y})}{\partial x_{j}} \ge 0,
$$
which contradicts (\ref{eq:b}).
\end{proof}