We use volatility as risk measure of the fully invested portfolio, i.e.
\begin{equation}
\mathcal{R}(x) = \sigma(x) = \sqrt{x^T Q x}
\end{equation}
where $Q$ is the covariance matrix. Using the Euler decomposition, we can express the total risk as the sum of contributions from each asset in the portfolio:
\begin{equation}
\mathcal{R}(x) = \sum_{i=1}^n RC_i 
\end{equation}
where $RC_i$ is the risk contribution of the $i$-th asset, that has the form
\begin{equation}
RC_i = x_i \frac{\partial \mathcal{R}(x)}{\partial x_i}
\end{equation}
In the Risk Parity formulation, our aim is to satisfy the following set of constraints:
\begin{equation}\label{eq:rpconst}
x_i \frac{\partial \mathcal{R}(x)}{\partial x_i}= x_j \frac{\partial \mathcal{R}(x)}{\partial x_j} \quad \forall i,j
\end{equation}
We can also express (\ref{eq:rpconst}) in the following equivalent way:
\begin{equation}
x_i (Q x)_i = x_j (Q x)_j \quad \forall i,j
\end{equation}
In \cite{maillard} is proposed a least-square approach for solving the Risk Parity problem:
\begin{subequations}
\begin{align}
\min_x & \quad \sum_{i=i}^n \sum_{j=1}^{n}\left(x_i(Q x)_i - x_j(Q x)_j\right)^2\\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathds{1}^T x = 1 
\end{align}
\end{subequations}
The formulation proposed in \cite{tutuncu} introduces a free variable $y$ that is also optimized:
\begin{subequations}\label{eq:problem} 
\begin{align}
\min_{x,y} & \quad f(x,y) =  \sum_{i=i}^n \left(x_i(Q x)_i - y\right)^2 \\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathds{1}^T x = 1 
\end{align}
\end{subequations}
where $x \in \R^n$, $y \in \R$, $f$ continuously differentiable, $l, u \in \R^n$ with $l < u$ and $\mathds{1} \in \R^n$ is all composed by ones. \\
Note that the formulation (\ref{eq:problem}) is non-convex.  If the optimization problem above has an optimal value of zero, then the RP portfolio is achieved. Otherwise, the value of $\sum_{i=i}^n \left( x_i(Q x)_i - y\right)^2$  can be regarded as a minimum variance measure towards our goal. Note that, since (\ref{eq:problem}) is a non-convex problem, in theory it is hard to solve and may produce local solutions. Anyway, we have the following lemma (for the proof see \cite{tutuncu}):
\begin{lemma}\label{rplemma}
Let $f(x,y) = \sum_{i=i}^n \left( x_i(Q x)_i - y \right)^2$. A solution pair $\{x,y\}$ is a global optimum with $f(x,y)=0$ if and only if $\nabla_xf(x,y) = 0$ and $\frac{\partial f(x,y)}{\partial y} = 0$.
\end{lemma}
Lemma (\ref{rplemma}) implies that if constraints of (\ref{eq:problem}) are not considered, then the first order optimality conditions determine the global optimal solution. On the other hand, when constraints are imposed, local optima and local stationary points can occur.
Now we define the feasible set $\mathcal{F}$  of Problem (\ref{eq:problem}):
\begin{equation}
\mathcal{F} = \{(x,y) \in \R^{n+1} : \mathds{1}^T x = 1, l \leq x \leq u\}.
\end{equation}
Since the constraints of Problem (\ref{eq:problem}) respect constraints qualification conditions, a point $(x,y) \in \mathcal{F}$ is critical, if the Karush-Kuhn-Tucker (KKT) conditions are satisfied.

Let $L(x,y,\lambda,\mu,\gamma)$ the Lagrangian function associated to Problem (\ref{eq:problem}) then we can write KKT conditions.

\begin{proposition}[Optimality conditions (Necessary)]\label{prop:KKT}

Let $(x^*,y^*) \in \R^{n+1}$, with $(x^*,y^*) \in \mathcal{F}$, a local optimum for Problem (\ref{eq:problem}). Then there exist three multipliers $\lambda^* \in \R^n$, $\mu^* \in \R^n, \gamma^* \in \R$ such that:
\begin{equation}
 \begin{aligned}
  &\nabla_x L(x^*,y^*\lambda^*,\mu^*,\gamma^*)= \nabla_x f(x^*,y^*)+\lambda^*-\mu^*+\gamma^*=0\\
 &\nabla_y L(x^*,y^*,\lambda^*,\mu^*,\gamma^*)=\nabla_y f(x^*,y^*) =0 \\
    &\lambda^*_i(l_i-x_i^*)=0,\ \forall i\\
 &\mu^*_i(x_i^*-u_i)=0,\ \forall i\\
   & \lambda^*,\mu^*\ge0 \\
 \end{aligned}
\end{equation}
\end{proposition}

From the first condition we have:
\begin{equation}
 \nabla_x f(x^*,y^*)-\lambda^*+\mu^*+\gamma^*=0
\end{equation}

Then there are three possible cases:
\begin{equation}
 \frac{\partial f(x^*,y^*)}{dx_i} = \begin{cases} -\mu_i^* -\gamma^* \hspace{1cm} x^*_i =u \\
 -\gamma^*+\lambda^*_i \hspace{1cm} x^*_i =l \\
 -\gamma^* \hspace{1.65cm} l<x^*_i <u 
\end{cases}
\end{equation}
Then if $x^*_i>l_i$: 
\begin{equation}
 \frac{\partial f(x^*,y^*)}{dx_i} \le \frac{\partial f(x^*,y^*)}{dx_h}, \forall h
\end{equation}

After writing KKT optimality condition we focus on feasible direction in a feasibile point.

Let $(x,y) \in \mathcal{F}$, we define a set of all feasible direction in $(x,y)$:
\begin{equation}
 \mathcal{D}(x,y)=\{ d \in \R^{n+1}: \mathds{1}^Td_x=0, d_i\ge 0 \ \forall i \in L(x), d_i\le 0 \ \forall i \in U(x)\}
\end{equation}
where:
\begin{equation}
 \begin{aligned}
  &L(x)=\{ i: \ x_i=l_i\}\\
  &U(x)=\{ i: \ x_i=u_i\}
 \end{aligned}
\end{equation}

and $d_x \in \R^n$ represent direction $d$ with respect to $x$ variable.

\subsection{Set of sparse feasible directions}
Because of we will describe our decomposition method with respect to $x$ and $y$, we have to pay attention only on the $x$ variable in order to find a feasible descent direction in $(x,y)$.
Then when we will talk about line search algorithm, we always refer to $x$ variable.

In our case we want to build a set of sparse feasible direction in order to justify our decomposition approach.

Let $(x,y) \in \mathcal{F}$ non stationary w.r.t. $x$, then it's easy to see that
\begin{equation}
 L(x)\ne \{1,\ldots,n\}
\end{equation}
hence $\exists i,j$ such that $x_j>l_j$ and $i \ne j$ such that:
\begin{equation}
 \frac{\partial f(x)}{dx_j} > \frac{\partial f(x)}{dx_i}, 
\end{equation}

Now we define a direction $d^{i,j} \in \R^n$ with only two non-zero components such that:
\begin{equation}\label{eq:direction}
d_h^{i,j}= 
\begin{cases}
1, \quad \text{    } h=i\\
-1, \text{    } \text{    } h=j\\
0, \quad \text{    } \text{otherwise}
\end{cases}
\end{equation}

\begin{proposition}
Let $(x,y) \in \mathcal{F}$, then the direction $d^{i,j}$ is a feasible and descent direction in $x$.
\end{proposition}
\begin{proof}
For the feasibility it is enough to see that $\mathds{1}^Td^{i,j}=1-1=0$.
Then we can apply sufficient conditions for descent direction in $x$, such that:
\begin{equation*}
 \nabla_xf(x,y)^Td^{i,j} =  \frac{\partial f(x,y)}{dx_i} - \frac{\partial f(x,y)}{dx_j}<0; 
\end{equation*}
\end{proof}

As always, we should choose the steepest descent direction composed by only two non-zero components.
This can be done computing the \emph{Most Violating Pair} $(i,j)$ such that $x_j>l_j$ and:
\begin{equation}
 (i,j) \in \arg \min_{l,m} \left\{\frac{\partial f(x,y)}{dx_l} - \frac{\partial f(x,y)}{dx_m}  \right\}
\end{equation}

If we don't use decomposition methods, we can alternatively define a feasible descent direction $d \in \R^{n+1}$, in $(x,y)$ such that:
\begin{equation}
 d=\left(\begin{matrix}d^{i,j}\\
 -\nabla_yf(x,y)
   \end{matrix}\right)
\end{equation}
\iffalse
\subsection{Armijo-Type Line Search Algorithm}
In this section, we briefly describe the well-known Armijo-type line search along a feasible descent direction. The procedure will be used in the decomposition method presented in the next section. 
Let $d^{k} \in \mathcal{D}(x_k)$  $x^{k} \in \mathcal{F}$. In particular we choose $d^{k}=d^{i,j}_k$ with MVP $(i(k),j(k))$.
We denote by $\Delta_{k}$ the maximum feasible step along $d^{k}$. 

It is easy to see that:
\begin{equation*}
\Delta_k=\min \{ x^k_{j(k)}-l, u-x^k_{i(k)}\}
\end{equation*}
\begin{algorithm}[ht]
 \KwData{Given $\alpha > 0$, $\delta \in (0,1)$, $\gamma \in (0, 1/2)$ and the initial stepsize $\Delta^{(k)} =\min \{ x^k_{j(k)}-l, u-x^k_{i(k)}\}$ }
 %\KwResult{A feasible step $\lambda$}
 Set $\alpha = \Delta^{(k)}$\\
 \While{$f(x^{k},y^k) + \alpha d^{k}) > f(x^{k},y^k) + \gamma \alpha \nabla_x f(x^{k},y^k)^T d^{k}$}{
  Set $\alpha = \delta \alpha$
 }
 \caption{Armijo-Type Line Search}
\end{algorithm}

Then at iteration $k+1$ we have:
\begin{equation*}
x^{k+1}_{j(k)}=\begin{cases}
 l \ &\alpha_k=x^k_{j(k)}-l\\
 x^k_{j(k)}-u+x^k_{i(k)} \ &\alpha_k=u-x^k_{i(k)}
 \end{cases}
\end{equation*}
and:
\begin{equation*}
x^{k+1}_{i(k)}=\begin{cases}
 x^k_{j(k)}-l+x^k_{i(k)} \ &\alpha_k=x^k_{j(k)}-l\\
 u \ &\alpha_k=u-x^k_{i(k)}
 \end{cases}
\end{equation*}




\begin{proposition}\label{proposition:david}
 If we apply Armijo type line-search, using MVP and a descent direction $d_k^{i,j}$ then exists $N \in \N$ such that at most after $N$ consecutive iterations, $\alpha_k<\Delta_k$,i.e.:
 \begin{equation}
  a^{k+M}<\Delta^{k+M}
 \end{equation}
\end{proposition}
\begin{proof}
First of all we have to consider $l_i = l, u_i=u, \forall i$ and we define $l-norm (u-norm)$ of a vector, such that:
\begin{equation*}
\begin{aligned}
 ||x||_l:= |\{x_i| x_i >l\}| \\
 ||x||_u:= |\{x_i| x_i <u\}| 
 \end{aligned}
\end{equation*}
\end{proof}

The maximum feasible step at every iteration is
\begin{equation}
\Delta_k= \min \{ x^k_{j(k)}-l, u-x^k_{i(k)}\}
\end{equation}

Then, once renaming $i(k)=i,j(k)=j$, at iteration $k+1$ we have:
\begin{equation*}
x^{k+1}_{j}=\begin{cases}
 l \ &x^k_{i}\le u+l-x^k_{j}\\
 x^k_{j}-u+x^k_{i} \ &x^k_{i}> u+l-x^k_{j}
 \end{cases}
\end{equation*}
and:
\begin{equation*}
x^{k+1}_{i}=\begin{cases}
 x^k_{j}-l+x^k_{i} \ &x^k_{i}\le u+l-x^k_{j}\\
 u \ &x^k_{i}> u+l-x^k_{j}
 \end{cases}
\end{equation*}

Hence we have the follwing cases:
\begin{equation}
 ||x^{k+1}||_l=\begin{cases} ||x_{k}||_l\hspace{2cm} x^k_{i}> u+l-x^k_{j} \\
 ||x_k||_l-1\hspace{2cm} else               
              \end{cases}
\end{equation}
and:
\begin{equation}
 ||x^{k+1}||_u=\begin{cases} ||x_{k}||_u\hspace{2cm} x^k_{i}\le u+l-x^k_{j} \\
 ||x_k||_u-1\hspace{2cm} else               
              \end{cases}
\end{equation}

Because of the sequences of $||\cdot||_l, ||\cdot||_u$ are not increasing.
Indeed the two first cases hold for at most $2n!$ consecutive iteration (they are simple permutation of vector $x^k$ ) and the two second cases hold for at most $n-1$ consecutive iterations,
we can conclude that there is a number $M(k) \in \mathbb{N}$ such that:
\begin{equation}
 \alpha_{k+M(k)} < \Delta_{k+M(k)}
\end{equation}
and $M(k)\le 2(n-1)n!$.
\fi
\subsection{Quadratic Line Search (QLS)}
In order to find a feasible step along a descent direction in $x^k$, we use a line search method called quadratic line search.

It's easy to see that the maximum feasible step at every iteration $k$ is:
\begin{equation}
\Delta_k= \min \{ x^k_{j(k)}-l_{j(k)}, u_{i(k)}-x^k_{i(k)}\}
\end{equation}
The QLS algorithm procedure (algorithm \ref{alg:QLS}) starts from $\alpha_k = \Delta_k$ and decreases $\alpha_k$ until:
\begin{equation}
f(x_k+\alpha_kd_k) \le  f(x_k)- \gamma (\alpha_k||d_k||)^2
\end{equation}
 where $d_k$ is a descent direction in $x_k$.



Altough the most famous line search method is Armijo-Type, its most important drawback is that it can't guarantee that:
\begin{equation}
 \displaystyle \lim_{k\rightarrow \infty} ||x^{k+1}-x^{k}|| =0
\end{equation}



 
 \begin{algorithm}[ht]
 \KwData{Given $\alpha > 0$, $\delta \in (0,1)$, $\gamma \in (0, 1/2)$ and the initial stepsize $\Delta^{k} =\min \{ x^k_{j(k)}-l, u-x^k_{i(k)}\}$ }
 %\KwResult{A feasible step $\lambda$}
 Set $\alpha = \Delta^{k}$\\
 \While{$f(x^{k},y^k) + \alpha d^{k}) > f(x^{k},y^k) - \gamma \left(\alpha ||d^{k}||\right)^2$}{
  Set $\alpha = \delta \alpha$
 }
 \caption{QLS Line Search}\label{alg:QLS}
\end{algorithm}

QLS algorithm has also the same convergency properties \cite{sciandrone-galligari-dilorenzo} of Armijo-type.
\subsection{Exact Line Search (ELS)}
In our case when we move along the direction $d^{i(k),j(k)}$, defined in (\ref{eq:direction}), we modify only 2 variables ($x_{i(k)}, x_{j(k)}$) leaving the others unchanged. Thus, we can see our $f(x,y)$ as a function of two components, i.e. we can rewrite Problem (\ref{eq:problem}) as
\begin{subequations}\label{eq:twocomp} 
\begin{align}
\min_{x_{i(k)}, x_{j(k)}} & \quad f(x_{i(k)}, x_{j(k)})  \\
\text{s.t.} & \quad l_{i(k)} \leq x_{i(k)}  \leq u_{i(k)} \\
& \quad l_{j(k)} \leq x_{j(k)}  \leq u_{j(k)} \\
& \quad x_{i(k)}+x_{j(k)} = \underbrace{1-\sum_{h\ne {i(k)},{j(k)}}x_h}_c
\end{align}
\end{subequations}
Thanks to the last constraint, we can substitute $x_{i(k)} = c - x_{j(k)}$ and then we obtain
\begin{subequations}\label{eq:onecomp} 
\begin{align}
\min_{\xi} & \quad f(\xi) \\
\text{s.t.} & \quad x_{i(k)} = c - \xi \\
& \quad l_{\xi} \leq \xi \leq u_{\xi}
\end{align}
\end{subequations}
where:
\begin{equation}
\begin{aligned}
 l_{\xi} = \max\{l_{j(k)}, c - u_{i(k)}\}\\
 u_{\xi}= \min \{ u_{j(k)}, c-l_{i(k)}\}  
 \end{aligned}
\end{equation}

Because the domain is $I=[l_{\xi}, u_{\xi}]$, and $f(\xi)$ is continuous and differentiable in $I$, then $f$ has a minimum in $I$ and we can compute $f'(\xi)$. 

Let $R = \{ r \enskip | \enskip f'(r) = 0, r \in I \}$ be set set of the real feasible roots of $f'$. Each $r \in R$ can be a local maximum, minimum or flex; if $R = \{ \emptyset \}$, then the minimum of $f$ is on the extreme points of $I$.\\ 
Let $\xi^* =\displaystyle \argmin_{r \in R}\{ f(r)\}$, the optimal step $\alpha_k^*$ along the direction $d^{i(k),j(k)}$ is
\begin{equation}
\alpha_k^* = x_{j(k)} - \xi^* > 0
\end{equation} 
