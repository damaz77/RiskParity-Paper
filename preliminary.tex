Let us consider the following optimization problem:
\begin{subequations}\label{eq:problem} 
\begin{align}
\min_{x,y} & \quad f(x,y)  \\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathbf{1}^T x = 1 
\end{align}
\end{subequations}
where $x \in \mathbb{R}^n$, $y \in \mathbb{R}$, $f:\mathbb{R}^{n+1} \rightarrow \mathbb{R}$ is a \textit{TODO: which are the hypothesis on f?}, $l, u \in \mathbb{R}^n$ with $l < u$ and $\mathbf{1} \in \mathbb{R}^n$ is the identity vector. We indicate by $\mathcal{F}$ the feasible set of Problem (\ref{eq:problem}), namely
\begin{equation}
\mathcal{F} = \{x \in \mathbb{R}^n : \mathbf{1}^T x = 1, l \leq x \leq u\}.
\end{equation}
Since the constraints of Problem (\ref{eq:problem}) are linear, we have that a feasible point $(x,y)$ is a stationary point of Problem (\ref{eq:problem}) if and only if the Karush-Kuhn-Tucker (KKT) conditions are satisfied.

\begin{proposition}[TODO: Optimality conditions (Necessary)]\label{prop:KKT}
Let $(x^*,y^*) \in \mathbb{R}^{n+1}$, with $x^* \in \mathcal{F}$, a local optimum for Problem (\ref{eq:problem}). Then there are two multipliers $\lambda^* \in \mathbb{R}$, $\mu^* \in \mathbb{R}^n$ satisfying
\begin{subequations}
\begin{align}
\frac{\partial f(x^*, y^*)}{\partial y} = 0 \\
\frac{\partial f(x^*, y^*)}{\partial x_i} +\lambda^* - \mu^*_i =0 \\
\mu^*_ix^{*}_i=0 \\
\mu^*_i\ge0
\end{align}
\end{subequations}
\end{proposition}

\hspace{-1.8em} From Proposition (\ref{prop:KKT}) it follows that:

\begin{corollary}
If $(x^*, y^*) \in \mathbb{R}^{n+1}$ is a local optimum for Problem (\ref{eq:problem}), then
\begin{equation}
x_j^* > 0 \quad \Rightarrow \quad \frac{\partial f(x^*, y^*)}{\partial x_j} \leq \frac{\partial f(x^*, y^*)}{\partial x_i} \quad \forall i \in \{1, .., n\}
\end{equation}
\end{corollary}
Given a feasible point $(x,y)$, we define two indexes $i^*, j^* \in \{1, .., n\}$ in the following way:
\begin{subequations}\label{eq:gs}
\begin{align}
x_{i^*} < u_{i^*} \quad \text{and} \quad  \frac{\partial f(x,y)}{\partial x_{i^*}}\leq \frac{\partial f(x,y)}{\partial x_h} \quad h \text{ s.t. } x_h < u_h \\
x_{j^*} > l_{j^*} \quad \text{and} \quad  \frac{\partial f(x,y)}{\partial x_{j^*}}\geq \frac{\partial f(x,y)}{\partial x_h} \quad h \text{ s.t. } x_h > l_h 
\end{align}
\end{subequations}

\hspace{-1.8em} Now we define a direction $d^{i^*,j^*} \in \mathbb{R}^n$ with only two non-zero components such that:
\begin{equation}\label{eq:direction}
d^{i^*,j^*}_h = 
\begin{cases}
1, \quad \text{    } h=i^*\\
-1, \text{    } \text{    } h=j^*\\
0, \quad \text{    } \text{otherwise}
\end{cases}
\end{equation}

\begin{proposition}
Let $(x,y)$ a feasible point for Problem (\ref{eq:problem}). Then the direction $d^{i^*,j^*}$ is a descent direction for $f(x,y)$.
\end{proposition}
\begin{proof}
From Eq.(\ref{eq:gs}) and Eq.(\ref{eq:direction}) we can write:
\begin{equation}
\nabla_x f(x,y)^T d^{i^*,j^*} = \frac{\partial f(x, y)}{\partial x_i^*} - \frac{\partial f(x, y)}{\partial x_j^*} \leq 0
\end{equation}
\textit{TODO: Manca da dimostrare il minore secco}.
\end{proof}

\subsection{Armijo-Type Line Search Algorithm}
In this section, we describe the well-known Armijo-type line search along a feasible direction. The procedure will be used in the decomposition method presented in the next section. Let $d^{(k)}$ be a feasible direction at $(x^{(k)}, y^{(k)})$ with $x^{(k)} \in \mathcal{F}$. We denote by $\beta_{\mathcal{F}}^{(k)}$ the maximum feasible steplength along $d^{(k)}$, namely $\beta_{\mathcal{F}}^{(k)}$ satisfies
\begin{equation*}
l \leq x + \beta d^{(k)} \leq u \quad \text{for every} \quad \beta \in [ 0, \beta_{\mathcal{F}}^{(k)} ]
\end{equation*}
We have at least an index $i \in \{1, .., n\}$ such that
\begin{equation*}
x_i^{(k)} + \beta_{\mathcal{F}}^{(k)} d_i^{(k)} = l_i \qquad \text{or} \qquad x_i^{(k)} + \beta_{\mathcal{F}}^{(k)} d_i^{(k)} = u_i 
\end{equation*}
Let $\beta_u$ be a positive scalar and set
\begin{equation}
\beta^{(k)} = \min \{\beta_{\mathcal{F}}^{(k)}, \beta_u\}
\end{equation}

\hspace{-1.8em}An Armijo-type line search algorithm is described below.\\
\begin{algorithm}
 \KwData{Given $\alpha > 0$, $\delta \in (0,1)$, $\gamma \in (0, 1/2)$ and the initial stepsize $\alpha^{(k)} = \min \{\beta^{(k)},\alpha \}$}
 %\KwResult{A feasible step $\lambda$}
 Set $\lambda = \alpha^{(k)}$\\
 \While{$f(x^{(k)} + \lambda d^{(k)}) > f(x^{(k)}) + \gamma \lambda \nabla_x f(x^{(k)})^T d^{(k)}$}{
  Set $\lambda = \delta \lambda$
 }
 \caption{Armijo-Type Line Search}
\end{algorithm}

\subsection{Exact Line Search}
When we move along the direction $d^{i^*,j^*}$, defined in (\ref{eq:direction}), we modify only 2 variables ($x_{i^*}, x_{j^*}$) leaving the others unchanged. Thus, we can see our $f(x,y)$ as a function of two components, i.e. we can rewrite Problem (\ref{eq:problem}) as
\begin{subequations}\label{eq:twocomp} 
\begin{align}
\min_{x_{i^*}, x_{j^*}} & \quad f(x_{i^*}, x_{j^*})  \\
\text{s.t.} & \quad l_{i^*} \leq x_{i^*}  \leq u_{i^*} \\
& \quad l_{j^*} \leq x_{j^*}  \leq u_{j^*} \\
& \quad x_{i^*}+x_{j^*} = \underbrace{1-\sum_{h\ne {i^*},{j^*}}x_h}_c
\end{align}
\end{subequations}
Thanks to the last constraint, we can substitute $x_{i^*} = c - x_{j^*}$ and then we obtain
\begin{subequations}\label{eq:onecomp} 
\begin{align}
\min_{ x_{j^*}} & \quad f(x_{j^*}) \\
\text{s.t.} & \quad x_{i^*} = c - x_{j^*} \\
& \quad ll_{j^*} = \max\{l_{j^*}, c - u_{i^*}\} \leq x_{j^*} \leq \min \{ u_{j^*}, c-l_{i^*}\} = uu_{j^*}
\end{align}
\end{subequations}
Because the domain is $I=[ll_{j^*}, uu_{j^*}]$, if $f(x_{j^*})$ is continuous in $I$, then $f$ has a minimum in $I$. If $f(x_{j^*})$ is differentiable in $I$ we can compute $f'(x_{j^*})$. Let $R = \{ r \enskip | \enskip f'(r) = 0, r \in I \}$ be set set of the real feasible roots of $f'$. Each $r \in R$ can be a local maximum, minimum or flex; if $R = \{ \emptyset \}$, then the minimum of $f$ is on the extreme points of $I$.\\ 
Let $r* = \argmin_{r \in R} f(r)$, then the optimal step $\alpha^*$ along the direction $d^{i^*,j^*}$ is
\begin{equation}
\alpha^* = x_{j^*} - r^* > 0
\end{equation} 
\textit{TODO: problema di notazione, $x_{j^*}$ rappresenta il valore della componente $j^*$ del vettore $x$ prima della ricerca di linea}