Let us consider the following optimization problem:
\begin{subequations}\label{eq:problem} 
\begin{align}
\min_{x,y} & \quad f(x,y)  \\
\text{s.t.} & \quad l \leq x \leq u \\
& \quad \mathbf{1}^T x = 1 \\
& \quad x \geq 0
\end{align}
\end{subequations}
where $x \in \mathbb{R}^n$, $y \in \mathbb{R}$, $f:\mathbb{R}^{n+1} \rightarrow \mathbb{R}$ is a \textit{TODO: which are the hypothesis on f?}, $l, u \in \mathbb{R}^n$ with $l < u$ and $\mathbf{1} \in \mathbb{R}^n$ is the identity vector. We indicate by $\mathcal{F}$ the feasible set of Problem (\ref{eq:problem}), namely
\begin{equation}
\mathcal{F} = \{x \in \mathbb{R}^n : \mathbf{1}^T x = 1, l \leq x \leq u, x \geq 0 \}.
\end{equation}
Since the constraints of Problem (\ref{eq:problem}) are linear, we have that a feasible point $(x,y)$ is a stationary point of Problem (\ref{eq:problem}) if and only if the Karush-Kuhn-Tucker (KKT) conditions are satisfied.

\begin{proposition}[Optimality conditions (Necessary)]\label{prop:KKT}
Let $(x^*,y^*) \in \mathbb{R}^{n+1}$, with $x^* \in \mathcal{F}$, a local optimum for Problem (\ref{eq:problem}). Then there are two multipliers $\lambda^* \in \mathbb{R}$, $\mu^* \in \mathbb{R}^n$ satisfying
\begin{subequations}
\begin{align}
\frac{\partial f(x^*, y^*)}{\partial y} = 0 \\
\frac{\partial f(x^*, y^*)}{\partial x_i} +\lambda^* - \mu^*_i =0 \\
\mu^*_ix^{*}_i=0 \\
\mu^*_i\ge0
\end{align}
\end{subequations}
\end{proposition}

\hspace{-1.8em} From Proposition (\ref{prop:KKT}) it follows that:

\begin{corollary}
If $(x^*, y^*) \in \mathbb{R}^{n+1}$ is a local optimum for Problem (\ref{eq:problem}), then
\begin{equation}
x_j^* > 0 \quad \Rightarrow \quad \frac{\partial f(x^*, y^*)}{\partial x_j} \leq \frac{\partial f(x^*, y^*)}{\partial x_i} \quad \forall i \in \{1, .., n\}
\end{equation}
\end{corollary}
Given a feasible point $(x,y)$, we define two indexes $i^*, j^* \in \{1, .., n\}$ in the following way:
\begin{subequations}\label{eq:gs}
\begin{align}
x_{i^*} < u_{i^*} \quad \text{and} \quad  \frac{\partial f(x,y)}{\partial x_{i^*}}\leq \frac{\partial f(x,y)}{\partial x_h} \quad h \text{ s.t. } x_h < u_h \\
x_{j^*} > l_{j^*} \quad \text{and} \quad  \frac{\partial f(x,y)}{\partial x_{j^*}}\geq \frac{\partial f(x,y)}{\partial x_h} \quad h \text{ s.t. } x_h > l_h 
\end{align}
\end{subequations}

\hspace{-1.8em} Now we define a direction $d^{i^*,j^*} \in \mathbb{R}^n$ with only two non-zero components such that:
\begin{equation}\label{eq:direction}
d^{i^*,j^*}_h = 
\begin{cases}
1, \quad \text{    } h=i^*\\
-1, \text{    } \text{    } h=j^*\\
0, \quad \text{    } \text{otherwise}
\end{cases}
\end{equation}

\begin{proposition}
Let $(x,y)$ a feasible point for Problem (\ref{eq:problem}). Then the direction $d^{i^*,j^*}$ is a descent direction for $f(x,y)$.
\end{proposition}
\begin{proof}
From Eq.(\ref{eq:gs}) and Eq.(\ref{eq:direction}) we can write:
\begin{equation}
\nabla_x f(x,y)^T d^{i^*,j^*} = \frac{\partial f(x, y)}{\partial x_i^*} - \frac{\partial f(x, y)}{\partial x_j^*} \leq 0
\end{equation}
\textit{TODO: Manca da dimostrare il minore secco}.
\end{proof}

\subsection{Armijo-Type Line Search Algorithm}
In this section, we describe the well-known Armijo-type line search along a feasible direction [1]. The procedure will be used in the decomposition method presented in the next section. Let $d^{(k)}$ be a feasible direction at $(x^{(k)}, y^{(k)})$ with $x^{(k)} \in \mathcal{F}$. We denote by $\beta_{\mathcal{F}}^{(k)}$ the maximum feasible steplength along $d^{(k)}$, namely $\beta_{\mathcal{F}}^{(k)}$ satisfies
\begin{equation*}
l \leq x + \beta d^{(k)} \leq u \quad \text{for every} \quad \beta \in [ 0, \beta_{\mathcal{F}}^{(k)} ]
\end{equation*}
We have at least an index $i \in \{1, .., n\}$ such that
\begin{equation*}
x_i^{(k)} + \beta_{\mathcal{F}}^{(k)} d_i^{(k)} = l_i \qquad \text{or} \qquad x_i^{(k)} + \beta_{\mathcal{F}}^{(k)} d_i^{(k)} = u_i 
\end{equation*}
Let $\beta_u$ be a positive scalar and set
\begin{equation}
\beta^{(k)} = \min \{\beta_{\mathcal{F}}^{(k)}, \beta_u\}
\end{equation}

\hspace{-1.8em}An Armijo-type line search algorithm is described below.\\
\begin{algorithm}
 \KwData{Given $\alpha > 0$, $\delta \in (0,1)$, $\gamma \in (0, 1/2)$ and the initial stepsize $\alpha^{(k)} = \min \{\beta^{(k)},\alpha \}$}
 %\KwResult{A feasible step $\lambda$}
 \While{$f(x^{(k)} + \lambda d^{(k)}) > f(x^{(k)}) + \gamma \lambda \nabla_x f(x^{(k)})^T d^{(k)}$}{
  Set $\lambda = \delta \lambda$
 }
 \caption{Armijo-Type Line Search}
\end{algorithm}
